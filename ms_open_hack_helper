https://docs.docker.com/network/bridge/
>docker network create ms-open-hack-net


https://hub.docker.com/_/microsoft-mssql-server
https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker?view=sql-server-ver15&pivots=cs1-bash
>docker run -e "ACCEPT_EULA=Y" -e "SA_PASSWORD=MyStrongPassw0rd123" \
   -p 1433:1433 --name sql1 -h sql1 \
   --network ms-open-hack-net -d \
   mcr.microsoft.com/mssql/server:2019-latest
   
======sa -> MyStrongPassw0rd123========

After setting up a SQL Server container running locally, add sample data to the database using the following command:

```dockerfile
docker run --network ms-open-hack-net -e SQLFQDN=sql1 -e SQLUSER=sa -e SQLPASS=MyStrongPassw0rd123 -e SQLDB=mydrivingDB openhack/data-load:v1
```

poi
>docker build --no-cache --build-arg IMAGE_VERSION="1.0" --build-arg IMAGE_CREATE_DATE="`date -u +"%Y-%m-%dT%H:%M:%SZ"`" --build-arg IMAGE_SOURCE_REVISION="`git rev-parse HEAD`" -f Dockerfile -t "tripinsights/poi:1.0" .


Then configure the POI application to connect to this SQL Server so you can test that the application works. You can find the curl commands to test the applications endpoints in the POI applications README.
IMPORTANT: Set the ASPNETCORE_ENVIRONMENT environment variable in POI to Local. This configures the application to skip the use of SSL encryption, allowing connection to the local sql server.
>docker run --network ms-open-hack-net --rm -dit \
  -p 8080:80 --name poi -h poi \
  -e SQL_USER=sa -e SQL_PASSWORD=MyStrongPassw0rd123 \
  -e SQL_SERVER=sql1 -e ASPNETCORE_ENVIRONMENT=Local \
  tripinsights/poi:1.0

Trips API

Build
>docker build --no-cache --build-arg IMAGE_VERSION="1.0" --build-arg IMAGE_CREATE_DATE="`date -u +"%Y-%m-%dT%H:%M:%SZ"`" --build-arg IMAGE_SOURCE_REVISION="`git rev-parse HEAD`" -f Dockerfile -t "tripinsights/trips:1.0" .

Run
>docker run --network ms-open-hack-net --rm -dit \
  -p 8081:80 --name trips -h trips \
  -e SQL_USER=sa -e SQL_PASSWORD=MyStrongPassw0rd123 \
  -e SQL_SERVER=sql1 -e ASPNETCORE_ENVIRONMENT=Local \
  tripinsights/trips:1.0
  





tripviewer
>docker build --no-cache --build-arg IMAGE_VERSION="1.0" --build-arg IMAGE_CREATE_DATE="`date -u +"%Y-%m-%dT%H:%M:%SZ"`" --build-arg IMAGE_SOURCE_REVISION="`git rev-parse HEAD`" -f Dockerfile -t "tripinsights/tripviewer:1.0" .


userjava
>docker build --no-cache --build-arg IMAGE_VERSION="1.0" --build-arg IMAGE_CREATE_DATE="`date -u +"%Y-%m-%dT%H:%M:%SZ"`" --build-arg IMAGE_SOURCE_REVISION="`git rev-parse HEAD`" -f Dockerfile -t "tripinsights/userjava:1.0" .


userprofile
>docker build --no-cache --build-arg IMAGE_VERSION="1.0" --build-arg IMAGE_CREATE_DATE="`date -u +"%Y-%m-%dT%H:%M:%SZ"`" --build-arg IMAGE_SOURCE_REVISION="`git rev-parse HEAD`" -f Dockerfile -t "tripinsights/userprofile:1.0" .


>az login
>az acr login --name registryfgf3387

>docker tag tripinsights/poi:1.0 registryfgf3387.azurecr.io/poi:1.0
>docker push registryfgf3387.azurecr.io/poi:1.0

>docker tag tripinsights/tripviewer:1.0 registryfgf3387.azurecr.io/tripviewer:1.0
>docker push registryfgf3387.azurecr.io/tripviewer:1.0

>docker tag tripinsights/userjava:1.0 registryfgf3387.azurecr.io/userjava:1.0
>docker push registryfgf3387.azurecr.io/userjava:1.0

>docker tag tripinsights/trips:1.0 registryfgf3387.azurecr.io/trips:1.0
>docker push registryfgf3387.azurecr.io/trips:1.0

>docker tag tripinsights/userprofile:1.0 registryfgf3387.azurecr.io/userprofile:1.0
>docker push registryfgf3387.azurecr.io/userprofile:1.0



>kubectl create ns openhack
>kubectl -n openhack apply -f deploy_poi.yaml
>kubectl -n openhack port-forward <poi pod> 8080:80
>http://localhost:8080/api/docs/poi/index.html

>kubectl -n openhack apply -f deploy_trips.yaml
>kubectl -n openhack port-forward <trip pod> 8081:80
http://localhost:8081/api/docs/trips/?url=http://localhost:8081/api/json/swagger.json

>kubectl -n openhack apply -f deploy_userprofile.yaml
>kubectl -n openhack port-forward <userprofile pod> 8082:80
>http://localhost:8082/api/docs/user/

>kubectl -n openhack apply -f deploy_userjava.yaml
>kubectl -n openhack port-forward <userjava pod> 8083:80
>http://localhost:8083/api/documentation/user-java/swagger-ui.html (or http://localhost:8083/api/docs/user-java)

web_users e6ae5f37-e913-4151-80a4-2809b7a018bf
api_users 606a2dd8-1a9e-425f-b916-6c02d5ff972d



kubectl auth can-i create deployments --namespace web


csi driver
https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver#code-try-1
Register the AKS-AzureKeyVaultSecretsProvider feature flag by using the az feature register command, as shown in the following example:
>az feature register --namespace "Microsoft.ContainerService" --name "AKS-AzureKeyVaultSecretsProvider"
It takes a few minutes for the status to show Registered. Verify the registration status by using the az feature list command:
>az feature list -o table --query "[?contains(name, 'Microsoft.ContainerService/AKS-AzureKeyVaultSecretsProvider')].{Name:name,State:properties.state}"
When ready, refresh the registration of the Microsoft.ContainerService resource provider by using the az provider register command:
>az provider register --namespace Microsoft.ContainerService
# Install the aks-preview extension
az extension add --name aks-preview
# Update the extension to make sure you have the latest version installed
az extension update --name aks-preview
#To upgrade an existing AKS cluster with Secrets Store CSI Driver capability
az aks enable-addons --addons azure-keyvault-secrets-provider --name OpenHack_AKS_Team3 --resource-group teamResources
The above will install the Secrets Store CSI Driver and the Azure Key Vault provider on your nodes. Verify completion by listing all pods with the secrets-store-csi-driver and secrets-store-provider-azure labels in the kube-system namespace, and ensure your output looks similar to the following:
>kubectl get pods -n kube-system -l 'app in (secrets-store-csi-driver, secrets-store-provider-azure)'



https://docs.microsoft.com/en-us/azure/aks/ingress-basic
REGISTRY_NAME=registryfgf3387
CONTROLLER_REGISTRY=k8s.gcr.io
CONTROLLER_IMAGE=ingress-nginx/controller
CONTROLLER_TAG=v0.48.1
PATCH_REGISTRY=docker.io
PATCH_IMAGE=jettech/kube-webhook-certgen
PATCH_TAG=v1.5.1
DEFAULTBACKEND_REGISTRY=k8s.gcr.io
DEFAULTBACKEND_IMAGE=defaultbackend-amd64
DEFAULTBACKEND_TAG=1.5

az acr import --name $REGISTRY_NAME --source $CONTROLLER_REGISTRY/$CONTROLLER_IMAGE:$CONTROLLER_TAG --image $CONTROLLER_IMAGE:$CONTROLLER_TAG
az acr import --name $REGISTRY_NAME --source $PATCH_REGISTRY/$PATCH_IMAGE:$PATCH_TAG --image $PATCH_IMAGE:$PATCH_TAG
az acr import --name $REGISTRY_NAME --source $DEFAULTBACKEND_REGISTRY/$DEFAULTBACKEND_IMAGE:$DEFAULTBACKEND_TAG --image $DEFAULTBACKEND_IMAGE:$DEFAULTBACKEND_TAG


# Set the namespace to be used 
NAMESPACE=ingress-basic

# Add the ingress-nginx repository
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

# Set variable for ACR location to use for pulling images
ACR_URL=registryfgf3387.azurecr.io

# Use Helm to deploy an NGINX ingress controller
helm install nginx-ingress ingress-nginx/ingress-nginx \
    --namespace $NAMESPACE \
    --set controller.replicaCount=2 \
    --set controller.nodeSelector."kubernetes\.io/os"=linux \
    --set controller.image.registry=$ACR_URL \
    --set controller.image.image=$CONTROLLER_IMAGE \
    --set controller.image.tag=$CONTROLLER_TAG \
    --set controller.image.digest="" \
    --set controller.admissionWebhooks.patch.nodeSelector."kubernetes\.io/os"=linux \
    --set controller.admissionWebhooks.patch.image.registry=$ACR_URL \
    --set controller.admissionWebhooks.patch.image.image=$PATCH_IMAGE \
    --set controller.admissionWebhooks.patch.image.tag=$PATCH_TAG \
    --set defaultBackend.nodeSelector."kubernetes\.io/os"=linux \
    --set defaultBackend.image.registry=$ACR_URL \
    --set defaultBackend.image.image=$DEFAULTBACKEND_IMAGE \
    --set defaultBackend.image.tag=$DEFAULTBACKEND_TAG

